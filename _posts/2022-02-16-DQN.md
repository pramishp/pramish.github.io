---
layout: post
title: Implementing DQN using Pytorch for balancing pole on a cart (CartPole).
---

## Introduction:

The CartPole problem is a classic reinforcement learning problem where a pole is attached to a cart that can move in either direction along a frictionless track. The goal of the agent is to keep the pole balanced by applying appropriate forces to the cart. In this blog, we will use Deep Q-Networks (DQN) to solve the CartPole problem using PyTorch.

![_config.yml]({{ site.baseurl }}/images/cartpole.gif)

Take a look at DQN algorithm and start following the steps: 
```
1. Initialize replay memory capacity.
2. Initialize the main network and the target network with random weights.
3. For each episode:
   a. Initialize the environment.
   b. Initialize the state.
   c. For each time step:
   i. Select an action using an epsilon-greedy policy.
   ii. Take the action and observe the reward and the next state.
   iii. Store the transition in the replay memory.
   iv. Sample a batch of transitions from the replay memory.
   v. Calculate the Q-value targets using the Bellman equation.
   vi. Update the main network using stochastic gradient descent to minimize the mean squared error between the predicted Q-values and the Q-value targets.
   vii. Every C steps, copy the weights from the main network to the target network.
   viii. If the episode is terminated, break.
4. Output the trained main network.
```

## Code
<a href="https://github.com/pramishp/DQN-algorithm">DQN-algorithm</a>

## Step 1: Understanding DQN <br>
DQN is a neural network-based algorithm that can be used to learn policies for solving reinforcement learning problems. DQN works by learning the Q-values, which represent the expected future rewards for each action in a given state. The network is trained using a loss function that minimizes the difference between the Q-values predicted by the network and the actual Q-values observed during training.

## Step 2: Importing necessary libraries
We start by importing the necessary libraries such as PyTorch, OpenAI Gym, and NumPy. OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms.

```python
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
import random
```

## Step 3: Setting up the environment
The next step is to set up the CartPole environment using the gym.make() function. The reset() function initializes the state of the environment, and the step() function takes an action and returns the new state, reward, and whether the episode has ended.

```python
env = gym.make('CartPole-v0')
state_size = env.observation_space.shape[0]
action_size = env.action_space.n
```

## Step 4: Defining the DQN model
The DQN model is a neural network that takes the state as input and outputs Q-values for each action. We define a simple neural network with two fully connected layers using PyTorch.

```python
class DQN(nn.Module):
    def __init__(self, state_size, action_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, 24)
        self.fc2 = nn.Linear(24, 24)
        self.fc3 = nn.Linear(24, action_size)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
        
dqn = DQN(state_size, action_size)
```

## Step 5: Defining the replay memory
DQN uses experience replay to train the network. We define a replay memory using a deque data structure in Python. The replay memory stores experiences as tuples of (state, action, reward, next_state, done). The append() function adds an experience to the replay memory, and the popleft() function removes the oldest experience.

```python
class ReplayMemory:
    def __init__(self, capacity):
        self.memory = deque(maxlen=capacity)
    
    def append(self, experience):
        self.memory.append(experience)
        
    def sample(self, batch_size):
        batch = random.sample(self.memory, batch_size)
        states = torch.FloatTensor([experience[0] for experience in batch])
        actions = torch.LongTensor([experience[1] for experience in batch])
        rewards = torch.FloatTensor([experience[2] for experience in batch])
        next_states = torch.FloatTensor([experience[3] for experience in batch])
        dones = torch.FloatTensor([experience[4] for experience in batch])
        return states, actions, rewards, next_states, dones
        
memory = ReplayMemory(10000)
```

## Step 6: Defining the training
The DQN algorithm uses a process called "Bellman update" to update the Q-values in the network. The Bellman update is defined as follows:

```python
Q(state, action) = reward + gamma * max(Q(next_state, next_action))
```
where Q(state, action) is the Q-value for a given state and action, next_state is the state resulting from taking the action, next_action is the action with the highest Q-value in the next state, reward is the immediate reward for taking the action, and gamma is a discount factor that determines the importance of future rewards.

We define the training function that performs the Bellman update on a batch of experiences sampled from the replay memory. We also use a separate target network that is periodically updated with the weights of the main network.

```python
def train(memory, dqn, target_dqn, optimizer, batch_size, gamma):
    states, actions, rewards, next_states, dones = memory.sample(batch_size)
    Q = dqn(states).gather(1, actions.unsqueeze(1))
    Q_next = target_dqn(next_states).max(1)[0].detach()
    target = rewards + (1 - dones) * gamma * Q_next.unsqueeze(1)
    loss = F.mse_loss(Q, target)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

## Step 7: Setting up the training loop
We set up the main training loop that iteratively interacts with the environment, collects experiences, and updates the network weights.

```python
batch_size = 32
gamma = 0.99
epsilon = 1.0
epsilon_min = 0.01
epsilon_decay = 0.995
target_update = 10
optimizer = optim.Adam(dqn.parameters(), lr=0.001)

total_episodes = 500
max_steps = 200
scores = []
epsilons = []

for episode in range(total_episodes):
    state = env.reset()
    score = 0
    for step in range(max_steps):
        # Epsilon-greedy policy
        if random.random() > epsilon:
            action = dqn(torch.FloatTensor(state)).argmax().item()
        else:
            action = env.action_space.sample()
        next_state, reward, done, _ = env.step(action)
        memory.append((state, action, reward, next_state, done))
        state = next_state
        score += reward
        if len(memory.memory) > batch_size:
            train(memory, dqn, target_dqn, optimizer, batch_size, gamma)
        if step % target_update == 0:
            target_dqn.load_state_dict(dqn.state_dict())
        if done:
            break
    scores.append(score)
    epsilons.append(epsilon)
    epsilon = max(epsilon_min, epsilon_decay * epsilon)
```

## Step 8: Visualizing the results
We visualize the training progress by plotting the scores and epsilons over time using the matplotlib library.

```python
import matplotlib.pyplot as plt

fig, ax1 = plt.subplots()
color = 'tab:red'
ax1.set_xlabel('Episode')
ax1.set_ylabel('Score', color=color)
ax1.plot(scores, color=color)
ax2 = ax1.twinx()
color = 'tab:blue'
ax2.set_ylabel('Epsilon', color=color)
ax2.plot(epsilons, color=color)
fig.tight_layout()
plt.show()
```

## Conclusion:
In this blog, we used DQN to solve the CartPole problem using PyTorch. We went through each step of the implementation, from setting up the environment and defining the network to training the model and visualizing the results. We explained the intuition behind the DQN algorithm and how it can be applied to reinforcement learning problems. By following the steps outlined in this blog, you can gain a deeper understanding of how DQN works and how it can be implemented in practice. With this knowledge, you can apply DQN to other reinforcement learning problems and continue to explore the exciting field of deep reinforcement learning.
